{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ioi_utils import *\n",
    "from sae_variants import *\n",
    "from training import *\n",
    "from mandala._next.imports import *\n",
    "from mandala._next.common_imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circuit setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not find model\n"
     ]
    }
   ],
   "source": [
    "from circuit_utils import *\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "if 'model' in locals():\n",
    "    MODELS[MODEL_ID] = model\n",
    "\n",
    "HEAD_CLASS_FIG = {\n",
    "    'nm': 'Name Mover',\n",
    "    'bnm': 'Backup Name Mover',\n",
    "    'ind': 'Induction',\n",
    "    'nnm': 'Negative Name Mover',\n",
    "    'si': 'S-Inhibition',\n",
    "    'dt': 'Duplicate Token',\n",
    "    'pt': 'Previous Token',\n",
    "}\n",
    "\n",
    "COMPONENT_NAME_FIG = {\n",
    "    'k': 'Key',\n",
    "    'v': 'Value',\n",
    "    'q': 'Query',\n",
    "    'z': 'Attn Output',\n",
    "}\n",
    "\n",
    "CROSS_SECTION_FIG = {\n",
    "    'ind+dt@z': 'Ind+DT out',\n",
    "    'nm+bnm@q': '(B)NM q',\n",
    "    'nm+bnm@qk': '(B)NM qk',\n",
    "    'nm+bnm@z': '(B)NM out',\n",
    "    'si@v': 'S-I v',\n",
    "    'si@z': 'S-I out',\n",
    "}\n",
    "\n",
    "c = Circuit()\n",
    "paper_cross_sections = [\n",
    "    # IO\n",
    "    (c.zs(c.nm + c.bnm), ('io',), 'nm+bnm@z'),\n",
    "    (c.qs(c.nm + c.bnm) + c.ks(c.nm + c.bnm), ('io',), 'nm+bnm@qk'),\n",
    "    (c.qs(c.nm + c.bnm), ('io',), 'nm+bnm@q'),\n",
    "    # S\n",
    "    (c.qs(c.nm + c.bnm) + c.ks(c.nm + c.bnm), ('s',), 'nm+bnm@qk'),\n",
    "    (c.qs(c.nm + c.bnm), ('s',), 'nm+bnm@q'),\n",
    "    (c.vs(c.si), ('s',), 'si@v'),\n",
    "    (c.zs(c.si), ('s',), 'si@z'),\n",
    "    (c.zs(c.ind) + c.zs(c.dt), ('s',), 'ind+dt@z'),\n",
    "    # Pos\n",
    "    (c.qs(c.nm + c.bnm) + c.ks(c.nm + c.bnm), ('io_pos',), 'nm+bnm@qk'),\n",
    "    (c.qs(c.nm + c.bnm), ('io_pos',), 'nm+bnm@q'),\n",
    "    (c.zs(c.si), ('io_pos',), 'si@z'),\n",
    "    (c.vs(c.si), ('io_pos',), 'si@v'),\n",
    "    # Pos + S\n",
    "    (c.qs(c.nm + c.bnm) + c.ks(c.nm + c.bnm), ('io_pos', 's'), 'nm+bnm@qk'),\n",
    "    (c.zs(c.si), ('io_pos', 's'), 'si@z'),\n",
    "    (c.vs(c.si), ('io_pos', 's'), 'si@v'),\n",
    "    (c.zs(c.ind) + c.zs(c.dt), ('io_pos', 's'), 'ind+dt@z'),\n",
    "    (c.zs(c.ind) + c.zs(c.dt), ('io_pos',), 'ind+dt@z'),\n",
    "    # All\n",
    "    (c.qs(c.nm + c.bnm) + c.ks(c.nm + c.bnm), ('io', 'io_pos', 's'), 'nm+bnm@qk'),\n",
    "]\n",
    "\n",
    "locations_displaynames = {\n",
    "    'nm+bnm@z': '(B)NM out',\n",
    "    'nm+bnm@qk': '(B)NM qk',\n",
    "    'nm+bnm@q': '(B)NM q',\n",
    "    'si@v': 'S-I v',\n",
    "    'si@z': 'S-I out',\n",
    "    'ind+dt@z': 'Ind+DT out',\n",
    "}\n",
    "\n",
    "NODES = c.zs(c.nm + c.bnm) + c.qs(c.nm + c.bnm) + c.zs(c.si) + [n for n in c.vs(c.si) if n.seq_pos == 's2'] + c.zs(c.ind) + c.zs(c.dt) + c.ks(c.nm + c.bnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = '/media/amakelov/SanDisk1TB/paper_sprint/test.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage(db_path=DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amakelov/workspace/current/conda_envs/serimats/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "MODELS[MODEL_ID] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logging level to debug\n",
    "from mandala._next.common_imports import logger\n",
    "import logging\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 32.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# nodes = list(c.nodes.keys())\n",
    "# circuit_nodes = list(c.nodes.keys())\n",
    "\n",
    "with storage:\n",
    "\n",
    "    ############################################################################ \n",
    "    ### prompt dataset for training supervised features\n",
    "    ############################################################################ \n",
    "    P_train = generate_prompts(\n",
    "        distribution=full_distribution,\n",
    "        patterns=['ABB', 'BAB'],\n",
    "        prompts_per_pattern=10_000,\n",
    "        random_seed=0,\n",
    "    )\n",
    "    N_TRAIN = len(storage.unwrap(P_train))\n",
    "    ### activations for training supervised features\n",
    "    As_train = run_with_cache(\n",
    "        prompts=P_train, \n",
    "        nodes=NODES,\n",
    "        batch_size=100,\n",
    "        model_id=MODEL_ID,\n",
    "        verbose=True,\n",
    "    )\n",
    "    A_TRAIN_DICT = {node: A for node, A in zip(NODES, As_train)}\n",
    "\n",
    "    ### precompute the mean logit difference for clean training data\n",
    "    logits_train_clean = run_with_hooks(prompts=P_train, hooks=[], batch_size=200,)\n",
    "    CLEAN_LD_MEAN = (storage.unwrap(logits_train_clean)[:, 0] - storage.unwrap(logits_train_clean)[:, 1]).mean().item()\n",
    "\n",
    "    ### precompute the mean-ablated logit difference when ablating each node\n",
    "    A_TRAIN_MEAN_DICT = {node: get_dataset_mean(A) for node, A in A_TRAIN_DICT.items()}\n",
    "\n",
    "    MEAN_ABLATED_LD_DICT = {}\n",
    "    for node, A in A_TRAIN_DICT.items():\n",
    "        MEAN_ABLATED_LD_DICT[node] = compute_mean_ablated_lds(\n",
    "            node=node, prompts=P_train, A_mean=A_TRAIN_MEAN_DICT[node], batch_size=200,\n",
    "        )\n",
    "\n",
    "    ############################################################################ \n",
    "    ### prompt dataset for editing\n",
    "    ############################################################################ \n",
    "    N_NAMES = len(NAMES)\n",
    "    editing_base_distribution = copy.deepcopy(full_distribution)\n",
    "    editing_base_distribution.names = editing_base_distribution.names[:N_NAMES // 2]\n",
    "    editing_source_distribution = copy.deepcopy(full_distribution)\n",
    "    editing_source_distribution.names = editing_source_distribution.names[N_NAMES // 2:]\n",
    "\n",
    "    P_edit = generate_prompts(\n",
    "        distribution=editing_base_distribution,\n",
    "        patterns=['ABB', 'BAB'],\n",
    "        prompts_per_pattern=2500,\n",
    "        random_seed=1,\n",
    "    )\n",
    "    As_to_edit = run_with_cache(\n",
    "        prompts=P_edit, \n",
    "        nodes=NODES,\n",
    "        batch_size=100,\n",
    "        model_id=MODEL_ID,\n",
    "        verbose=True,\n",
    "    )\n",
    "    A_EDIT_DICT = {node: A for node, A in zip(NODES, As_to_edit)}\n",
    "\n",
    "    N_EDIT = len(storage.unwrap(P_edit))\n",
    "    N_NAMES_EDIT_SOURCE = len(editing_source_distribution.names)\n",
    "\n",
    "    ############################################################################ \n",
    "    ### Compute counterfactual prompts and activations\n",
    "    ############################################################################ \n",
    "    FEATURE_SUBSETS = [('io_pos',), ('s',), ('io',), ] # ('s', 'io_pos',), ('io', 'io_pos'), ('s', 'io',), ('io_pos', 's', 'io',), ]\n",
    "\n",
    "    CF_PROMPTS_DICT = {}\n",
    "    for feature_subset in FEATURE_SUBSETS:\n",
    "        CF_PROMPTS_DICT[feature_subset] = get_cf_prompts(\n",
    "            prompts=P_edit, \n",
    "            features=feature_subset,\n",
    "            io_targets=generate_name_samples(N_EDIT, editing_source_distribution.names[:N_NAMES_EDIT_SOURCE // 2]),\n",
    "            s_targets=generate_name_samples(N_EDIT, editing_source_distribution.names[N_NAMES_EDIT_SOURCE//2:]),     \n",
    "        )\n",
    "    ### Compute counterfactual activations\n",
    "    As_counterfactual = {}\n",
    "    for feature_subset, cf_prompts in tqdm(CF_PROMPTS_DICT.items()):\n",
    "        As_counterfactual[feature_subset] = run_with_cache(\n",
    "            prompts=cf_prompts, \n",
    "            nodes=NODES,\n",
    "            batch_size=100,\n",
    "            model_id=MODEL_ID,\n",
    "            verbose=True,\n",
    "        )\n",
    "    for feature_subset in As_counterfactual:\n",
    "        As_counterfactual[feature_subset] = {node: As_counterfactual[feature_subset][i] for i, node in enumerate(NODES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.cache_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing supervised features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 298.89it/s]\n"
     ]
    }
   ],
   "source": [
    "with storage:\n",
    "    SUPERVISED_FEATURES_DICT = {}\n",
    "    SUPERVISED_RECONSTRUCTIONS_DICT = {}\n",
    "    for node, A in tqdm(A_TRAIN_DICT.items()):\n",
    "        for eventually in ['independent',]: # 'coupled', 'names', ]:\n",
    "            for codes_type in ('mean',):  # 'mse'):\n",
    "                node_parametrization = get_parametrization(node=node, eventually=eventually, use_names= (eventually == 'names'))\n",
    "                node_features = FEATURE_CONFIGURATIONS[node_parametrization]\n",
    "                code_getter = get_mean_codes if codes_type == 'mean' else lambda features, A, prompts: train_mse_codes(features=features, A=A, prompts=prompts, manual_bias=True)\n",
    "                codes, reconstructions = code_getter(\n",
    "                    features=node_features,\n",
    "                    A=A,\n",
    "                    prompts=P_train,\n",
    "                )\n",
    "                SUPERVISED_FEATURES_DICT[(node, node_parametrization, codes_type)] = codes\n",
    "                SUPERVISED_RECONSTRUCTIONS_DICT[(node, node_parametrization, codes_type)] = reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define a uniform schedule for all training runs\n",
    "\n",
    "# use exponentially spread-out checkpoints for the very early stages of training\n",
    "# measure right before resampling, as well as in the middle between resamplings\n",
    "# measure before and after the final LR decay\n",
    "# use two resampling stages, as it seems effects diminish after the first one\n",
    "CHECKPOINT_STEPS = [0, 1, 2, 4, 8, 16, 32, 64, 128, 500, 750, 1000, 1250, 1500, 2000]\n",
    "RESAMPLE_EPOCHS = [501, 1001, ]\n",
    "FINAL_DECAY_START = 1500 # decay the LR for the last 25% of training\n",
    "FINAL_DECAY_END = 2000\n",
    "\n",
    "### define hyperparam grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l2_loss: 652.0528, l1_loss: 183.6407, l0_loss: 241.2733, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00,  9.14it/s]\n",
      "l2_loss: 371.7496, l1_loss: 145.6902, l0_loss: 212.7557, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 10.73it/s]\n",
      "l2_loss: 123.3874, l1_loss: 88.7050, l0_loss: 156.9462, frac_dead: 0.0000: 100%|██████████| 2/2 [00:00<00:00, 10.67it/s]\n",
      "l2_loss: 25.4143, l1_loss: 32.8909, l0_loss: 77.8878, frac_dead: 0.0000: 100%|██████████| 4/4 [00:00<00:00,  9.86it/s] \n",
      "l2_loss: 43.6559, l1_loss: 8.1220, l0_loss: 27.3317, frac_dead: 0.0000: 100%|██████████| 8/8 [00:00<00:00, 11.35it/s] \n",
      "l2_loss: 55.7745, l1_loss: 2.7885, l0_loss: 10.7507, frac_dead: 0.0000: 100%|██████████| 16/16 [00:01<00:00, 11.33it/s]\n",
      "l2_loss: 41.0231, l1_loss: 3.6119, l0_loss: 7.7955, frac_dead: 0.0000: 100%|██████████| 32/32 [00:02<00:00, 11.32it/s]\n",
      "l2_loss: 18.2599, l1_loss: 4.7588, l0_loss: 3.4555, frac_dead: 0.0000: 100%|██████████| 64/64 [00:05<00:00, 11.92it/s]\n",
      "l2_loss: 14.0565, l1_loss: 4.7452, l0_loss: 1.4660, frac_dead: 0.3457: 100%|██████████| 372/372 [00:34<00:00, 10.87it/s]\n",
      "l2_loss: 13.2716, l1_loss: 4.8126, l0_loss: 2.4666, frac_dead: 0.0781: 100%|██████████| 250/250 [00:21<00:00, 11.46it/s] \n",
      "l2_loss: 13.7825, l1_loss: 4.7453, l0_loss: 1.9897, frac_dead: 0.0977: 100%|██████████| 250/250 [00:21<00:00, 11.58it/s]\n",
      "l2_loss: 13.6241, l1_loss: 4.7643, l0_loss: 1.8311, frac_dead: 0.1074: 100%|██████████| 250/250 [00:22<00:00, 11.07it/s] \n",
      "l2_loss: 13.7091, l1_loss: 4.7481, l0_loss: 1.4111, frac_dead: 0.2676: 100%|██████████| 250/250 [00:22<00:00, 11.33it/s]\n",
      "l2_loss: 13.6850, l1_loss: 4.7511, l0_loss: 1.3680, frac_dead: 0.2812: 100%|██████████| 500/500 [00:43<00:00, 11.50it/s]\n",
      "l2_loss: 620.9363, l1_loss: 179.7901, l0_loss: 236.6348, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 11.66it/s]\n",
      "l2_loss: 311.1192, l1_loss: 133.8248, l0_loss: 198.2023, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 11.72it/s]\n",
      "l2_loss: 88.6115, l1_loss: 74.3596, l0_loss: 133.9581, frac_dead: 0.0000: 100%|██████████| 2/2 [00:00<00:00,  9.65it/s] \n",
      "l2_loss: 26.0146, l1_loss: 26.5480, l0_loss: 62.7553, frac_dead: 0.0000: 100%|██████████| 4/4 [00:00<00:00, 13.12it/s]\n",
      "l2_loss: 45.1581, l1_loss: 7.2726, l0_loss: 23.0699, frac_dead: 0.0000: 100%|██████████| 8/8 [00:00<00:00, 11.74it/s] \n",
      "l2_loss: 55.9985, l1_loss: 2.7362, l0_loss: 10.1333, frac_dead: 0.0000: 100%|██████████| 16/16 [00:01<00:00, 11.44it/s]\n",
      "l2_loss: 44.3554, l1_loss: 3.2545, l0_loss: 7.8691, frac_dead: 0.0000: 100%|██████████| 32/32 [00:02<00:00, 10.96it/s]\n",
      "l2_loss: 22.4269, l1_loss: 4.2585, l0_loss: 3.8813, frac_dead: 0.0000: 100%|██████████| 64/64 [00:05<00:00, 10.99it/s]\n",
      "l2_loss: 15.4935, l1_loss: 4.3589, l0_loss: 1.5554, frac_dead: 0.4141: 100%|██████████| 372/372 [00:32<00:00, 11.42it/s]\n",
      "l2_loss: 14.2714, l1_loss: 4.4750, l0_loss: 2.7330, frac_dead: 0.0879: 100%|██████████| 250/250 [00:22<00:00, 11.07it/s]  \n",
      "l2_loss: 14.4796, l1_loss: 4.4230, l0_loss: 2.2184, frac_dead: 0.0918: 100%|██████████| 250/250 [00:23<00:00, 10.86it/s]\n",
      "l2_loss: 14.3230, l1_loss: 4.4342, l0_loss: 2.3169, frac_dead: 0.0117: 100%|██████████| 250/250 [00:21<00:00, 11.45it/s] \n",
      "l2_loss: 14.3486, l1_loss: 4.4255, l0_loss: 2.1037, frac_dead: 0.0176: 100%|██████████| 250/250 [00:21<00:00, 11.51it/s]\n",
      "l2_loss: 14.3057, l1_loss: 4.4304, l0_loss: 2.0638, frac_dead: 0.0195: 100%|██████████| 500/500 [00:46<00:00, 10.73it/s]\n",
      "l2_loss: 601.1042, l1_loss: 179.5012, l0_loss: 237.6702, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n",
      "l2_loss: 320.4075, l1_loss: 137.7551, l0_loss: 203.7753, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 11.65it/s]\n",
      "l2_loss: 96.6157, l1_loss: 79.4090, l0_loss: 142.6185, frac_dead: 0.0000: 100%|██████████| 2/2 [00:00<00:00, 10.13it/s]\n",
      "l2_loss: 24.6298, l1_loss: 28.1850, l0_loss: 67.9375, frac_dead: 0.0000: 100%|██████████| 4/4 [00:00<00:00, 11.45it/s]\n",
      "l2_loss: 44.9833, l1_loss: 7.3166, l0_loss: 24.2473, frac_dead: 0.0000: 100%|██████████| 8/8 [00:00<00:00, 11.54it/s] \n",
      "l2_loss: 56.3558, l1_loss: 2.7148, l0_loss: 10.3809, frac_dead: 0.0000: 100%|██████████| 16/16 [00:01<00:00, 11.87it/s]\n",
      "l2_loss: 45.1561, l1_loss: 3.2020, l0_loss: 7.7934, frac_dead: 0.0000: 100%|██████████| 32/32 [00:02<00:00, 11.64it/s]\n",
      "l2_loss: 25.7425, l1_loss: 4.1871, l0_loss: 4.0682, frac_dead: 0.0000: 100%|██████████| 64/64 [00:05<00:00, 11.73it/s]\n",
      "l2_loss: 18.2156, l1_loss: 4.5151, l0_loss: 2.1250, frac_dead: 0.4102: 100%|██████████| 372/372 [00:42<00:00,  8.85it/s]\n",
      "l2_loss: 17.4099, l1_loss: 4.5924, l0_loss: 3.6176, frac_dead: 0.0605: 100%|██████████| 250/250 [00:23<00:00, 10.73it/s]  \n",
      "l2_loss: 17.3720, l1_loss: 4.5750, l0_loss: 3.3855, frac_dead: 0.0938: 100%|██████████| 250/250 [00:23<00:00, 10.73it/s]\n",
      "l2_loss: 17.2118, l1_loss: 4.5900, l0_loss: 3.5330, frac_dead: 0.0156: 100%|██████████| 250/250 [00:25<00:00,  9.89it/s]\n",
      "l2_loss: 17.2079, l1_loss: 4.5858, l0_loss: 3.3944, frac_dead: 0.0156: 100%|██████████| 250/250 [00:23<00:00, 10.83it/s]\n",
      "l2_loss: 17.1657, l1_loss: 4.5907, l0_loss: 3.3396, frac_dead: 0.0156: 100%|██████████| 500/500 [00:45<00:00, 10.91it/s]\n",
      "l2_loss: 575.3278, l1_loss: 169.4114, l0_loss: 230.4526, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 12.78it/s]\n",
      "l2_loss: 214.5633, l1_loss: 108.7328, l0_loss: 171.1740, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 10.59it/s]\n",
      "l2_loss: 53.4198, l1_loss: 51.0238, l0_loss: 98.3906, frac_dead: 0.0000: 100%|██████████| 2/2 [00:00<00:00, 10.59it/s]\n",
      "l2_loss: 35.2475, l1_loss: 18.1258, l0_loss: 43.2178, frac_dead: 0.0000: 100%|██████████| 4/4 [00:00<00:00, 11.82it/s]\n",
      "l2_loss: 50.5525, l1_loss: 6.5257, l0_loss: 17.8569, frac_dead: 0.0039: 100%|██████████| 8/8 [00:00<00:00, 11.16it/s] \n",
      "l2_loss: 49.9509, l1_loss: 4.1693, l0_loss: 10.0371, frac_dead: 0.0039: 100%|██████████| 16/16 [00:01<00:00, 12.36it/s]\n",
      "l2_loss: 43.4522, l1_loss: 3.6385, l0_loss: 6.7975, frac_dead: 0.0000: 100%|██████████| 32/32 [00:02<00:00, 11.86it/s]\n",
      "l2_loss: 31.2384, l1_loss: 3.1642, l0_loss: 5.0019, frac_dead: 0.0020: 100%|██████████| 64/64 [00:05<00:00, 11.82it/s]\n",
      "l2_loss: 18.6175, l1_loss: 3.1177, l0_loss: 1.7980, frac_dead: 0.3555: 100%|██████████| 372/372 [00:32<00:00, 11.37it/s]\n",
      "l2_loss: 18.0966, l1_loss: 3.2091, l0_loss: 2.6808, frac_dead: 0.0371: 100%|██████████| 250/250 [00:21<00:00, 11.59it/s]    \n",
      "l2_loss: 17.4542, l1_loss: 3.2070, l0_loss: 2.3087, frac_dead: 0.1152: 100%|██████████| 250/250 [00:21<00:00, 11.68it/s]\n",
      "l2_loss: 17.1840, l1_loss: 3.2348, l0_loss: 2.1684, frac_dead: 0.0859: 100%|██████████| 250/250 [00:21<00:00, 11.55it/s]  \n",
      "l2_loss: 17.0615, l1_loss: 3.2435, l0_loss: 2.0362, frac_dead: 0.0938: 100%|██████████| 250/250 [00:21<00:00, 11.43it/s]\n",
      "l2_loss: 17.0134, l1_loss: 3.2492, l0_loss: 1.9935, frac_dead: 0.0938: 100%|██████████| 500/500 [00:43<00:00, 11.63it/s]\n",
      "l2_loss: 469.3934, l1_loss: 161.6836, l0_loss: 219.7661, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 11.86it/s]\n",
      "l2_loss: 117.2304, l1_loss: 93.1962, l0_loss: 153.6038, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 11.71it/s]\n",
      "l2_loss: 22.2656, l1_loss: 35.6973, l0_loss: 79.0529, frac_dead: 0.0039: 100%|██████████| 2/2 [00:00<00:00, 10.95it/s]\n",
      "l2_loss: 36.7835, l1_loss: 9.6486, l0_loss: 29.9216, frac_dead: 0.0117: 100%|██████████| 4/4 [00:00<00:00, 12.23it/s] \n",
      "l2_loss: 50.0765, l1_loss: 3.6054, l0_loss: 12.8448, frac_dead: 0.0156: 100%|██████████| 8/8 [00:00<00:00, 11.42it/s]\n",
      "l2_loss: 39.3464, l1_loss: 3.6995, l0_loss: 8.0252, frac_dead: 0.0195: 100%|██████████| 16/16 [00:01<00:00, 11.86it/s]\n",
      "l2_loss: 34.4079, l1_loss: 3.3547, l0_loss: 4.4568, frac_dead: 0.0215: 100%|██████████| 32/32 [00:02<00:00, 12.22it/s]\n",
      "l2_loss: 31.6794, l1_loss: 2.1007, l0_loss: 5.0175, frac_dead: 0.0098: 100%|██████████| 64/64 [00:05<00:00, 12.28it/s]\n",
      "l2_loss: 24.7624, l1_loss: 1.7352, l0_loss: 1.3895, frac_dead: 0.6602: 100%|██████████| 372/372 [00:31<00:00, 11.89it/s]\n",
      "l2_loss: 22.4902, l1_loss: 2.2360, l0_loss: 4.2263, frac_dead: 0.0137: 100%|██████████| 250/250 [00:20<00:00, 11.96it/s]      \n",
      "l2_loss: 18.1088, l1_loss: 2.5766, l0_loss: 1.9917, frac_dead: 0.1055: 100%|██████████| 250/250 [00:20<00:00, 11.94it/s]\n",
      "l2_loss: 21.3098, l1_loss: 2.3737, l0_loss: 2.7071, frac_dead: 0.4258: 100%|██████████| 250/250 [00:21<00:00, 11.85it/s]   \n",
      "l2_loss: 21.8839, l1_loss: 2.0900, l0_loss: 1.8457, frac_dead: 0.1719: 100%|██████████| 250/250 [00:21<00:00, 11.61it/s]\n",
      "l2_loss: 21.8819, l1_loss: 2.0902, l0_loss: 1.8026, frac_dead: 0.1855: 100%|██████████| 500/500 [00:43<00:00, 11.51it/s]\n",
      "l2_loss: 384.9647, l1_loss: 147.2532, l0_loss: 203.1339, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 12.02it/s]\n",
      "l2_loss: 118.8393, l1_loss: 85.6502, l0_loss: 136.8466, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 10.37it/s]\n",
      "l2_loss: 29.8851, l1_loss: 35.4113, l0_loss: 69.1518, frac_dead: 0.0020: 100%|██████████| 2/2 [00:00<00:00, 11.27it/s]\n",
      "l2_loss: 35.4686, l1_loss: 11.5357, l0_loss: 27.9045, frac_dead: 0.0059: 100%|██████████| 4/4 [00:00<00:00, 11.59it/s]\n",
      "l2_loss: 48.6230, l1_loss: 4.3483, l0_loss: 12.2797, frac_dead: 0.0098: 100%|██████████| 8/8 [00:00<00:00, 11.41it/s]\n",
      "l2_loss: 43.7976, l1_loss: 3.3844, l0_loss: 9.2423, frac_dead: 0.0059: 100%|██████████| 16/16 [00:01<00:00, 11.39it/s]\n",
      "l2_loss: 32.1621, l1_loss: 3.1733, l0_loss: 6.7159, frac_dead: 0.0059: 100%|██████████| 32/32 [00:02<00:00, 11.83it/s]\n",
      "l2_loss: 20.3729, l1_loss: 2.4981, l0_loss: 3.3281, frac_dead: 0.0020: 100%|██████████| 64/64 [00:05<00:00, 11.58it/s]\n",
      "l2_loss: 14.9372, l1_loss: 2.4862, l0_loss: 1.1758, frac_dead: 0.6328: 100%|██████████| 372/372 [00:32<00:00, 11.49it/s]\n",
      "l2_loss: 14.1813, l1_loss: 2.5716, l0_loss: 3.1858, frac_dead: 0.0332: 100%|██████████| 250/250 [00:21<00:00, 11.40it/s]   \n",
      "l2_loss: 13.9626, l1_loss: 2.6150, l0_loss: 2.4085, frac_dead: 0.0918: 100%|██████████| 250/250 [00:21<00:00, 11.54it/s]\n",
      "l2_loss: 13.7808, l1_loss: 2.6358, l0_loss: 2.2269, frac_dead: 0.1113: 100%|██████████| 250/250 [00:21<00:00, 11.72it/s]\n",
      "l2_loss: 13.7080, l1_loss: 2.6466, l0_loss: 1.9531, frac_dead: 0.1562: 100%|██████████| 250/250 [00:22<00:00, 11.34it/s]\n",
      "l2_loss: 13.6796, l1_loss: 2.6500, l0_loss: 1.9089, frac_dead: 0.1562: 100%|██████████| 500/500 [00:43<00:00, 11.48it/s]\n",
      "l2_loss: 414.3134, l1_loss: 152.7284, l0_loss: 210.7452, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 12.57it/s]\n",
      "l2_loss: 117.6289, l1_loss: 90.4567, l0_loss: 146.6043, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "l2_loss: 25.3393, l1_loss: 37.5406, l0_loss: 77.4196, frac_dead: 0.0000: 100%|██████████| 2/2 [00:00<00:00,  9.33it/s]\n",
      "l2_loss: 33.8743, l1_loss: 11.3862, l0_loss: 31.2529, frac_dead: 0.0020: 100%|██████████| 4/4 [00:00<00:00, 11.51it/s]\n",
      "l2_loss: 48.5248, l1_loss: 4.1869, l0_loss: 13.3762, frac_dead: 0.0156: 100%|██████████| 8/8 [00:00<00:00, 12.02it/s]\n",
      "l2_loss: 41.4920, l1_loss: 3.6262, l0_loss: 7.5118, frac_dead: 0.0059: 100%|██████████| 16/16 [00:01<00:00, 12.24it/s]\n",
      "l2_loss: 37.8995, l1_loss: 3.0127, l0_loss: 5.2645, frac_dead: 0.0117: 100%|██████████| 32/32 [00:02<00:00, 11.82it/s]\n",
      "l2_loss: 28.5850, l1_loss: 2.0561, l0_loss: 4.3867, frac_dead: 0.0059: 100%|██████████| 64/64 [00:05<00:00, 11.06it/s]\n",
      "l2_loss: 17.7629, l1_loss: 2.2256, l0_loss: 1.5978, frac_dead: 0.5508: 100%|██████████| 372/372 [00:34<00:00, 10.75it/s]\n",
      "l2_loss: 17.2324, l1_loss: 2.3062, l0_loss: 2.7559, frac_dead: 0.0469: 100%|██████████| 250/250 [00:26<00:00,  9.52it/s]   \n",
      "l2_loss: 16.4353, l1_loss: 2.3904, l0_loss: 2.6926, frac_dead: 0.2227: 100%|██████████| 250/250 [00:25<00:00,  9.80it/s]\n",
      "l2_loss: 16.2326, l1_loss: 2.4167, l0_loss: 2.5558, frac_dead: 0.1172: 100%|██████████| 250/250 [00:26<00:00,  9.42it/s]  \n",
      "l2_loss: 16.1652, l1_loss: 2.4242, l0_loss: 2.3181, frac_dead: 0.2344: 100%|██████████| 250/250 [00:23<00:00, 10.53it/s]\n",
      "l2_loss: 16.1264, l1_loss: 2.4298, l0_loss: 2.2710, frac_dead: 0.2363: 100%|██████████| 500/500 [00:48<00:00, 10.38it/s]\n",
      "l2_loss: 545.5354, l1_loss: 174.6871, l0_loss: 230.6073, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 10.74it/s]\n",
      "l2_loss: 243.1996, l1_loss: 123.7647, l0_loss: 185.1353, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 10.51it/s]\n",
      "l2_loss: 57.6774, l1_loss: 63.1412, l0_loss: 116.3127, frac_dead: 0.0000: 100%|██████████| 2/2 [00:00<00:00,  7.18it/s] \n",
      "l2_loss: 26.1467, l1_loss: 20.6469, l0_loss: 50.2589, frac_dead: 0.0000: 100%|██████████| 4/4 [00:00<00:00, 10.22it/s]\n",
      "l2_loss: 47.1448, l1_loss: 5.8234, l0_loss: 18.0991, frac_dead: 0.0000: 100%|██████████| 8/8 [00:00<00:00,  9.14it/s] \n",
      "l2_loss: 54.6726, l1_loss: 2.6165, l0_loss: 8.5410, frac_dead: 0.0000: 100%|██████████| 16/16 [00:01<00:00,  9.49it/s] \n",
      "l2_loss: 46.6112, l1_loss: 2.7726, l0_loss: 6.5610, frac_dead: 0.0020: 100%|██████████| 32/32 [00:03<00:00,  8.59it/s]\n",
      "l2_loss: 30.7631, l1_loss: 3.0767, l0_loss: 4.0954, frac_dead: 0.0020: 100%|██████████| 64/64 [00:06<00:00,  9.77it/s]\n",
      "l2_loss: 16.4193, l1_loss: 3.0497, l0_loss: 1.4460, frac_dead: 0.5195: 100%|██████████| 372/372 [00:37<00:00, 10.01it/s]\n",
      "l2_loss: 15.2824, l1_loss: 3.2277, l0_loss: 2.9760, frac_dead: 0.0762: 100%|██████████| 250/250 [00:23<00:00, 10.78it/s]   \n",
      "l2_loss: 15.2196, l1_loss: 3.1593, l0_loss: 2.5995, frac_dead: 0.0820: 100%|██████████| 250/250 [00:23<00:00, 10.73it/s]\n",
      "l2_loss: 15.0723, l1_loss: 3.1733, l0_loss: 2.5574, frac_dead: 0.0156: 100%|██████████| 250/250 [00:25<00:00,  9.76it/s]\n",
      "l2_loss: 15.0196, l1_loss: 3.1703, l0_loss: 2.3295, frac_dead: 0.0312: 100%|██████████| 250/250 [00:25<00:00,  9.74it/s]\n",
      "l2_loss: 14.9790, l1_loss: 3.1750, l0_loss: 2.2973, frac_dead: 0.0332: 100%|██████████| 500/500 [00:49<00:00, 10.04it/s]\n",
      "l2_loss: 497.7376, l1_loss: 165.5032, l0_loss: 224.0134, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 10.63it/s]\n",
      "l2_loss: 185.6954, l1_loss: 104.3268, l0_loss: 162.5046, frac_dead: 0.0000: 100%|██████████| 1/1 [00:00<00:00, 10.02it/s]\n",
      "l2_loss: 47.1494, l1_loss: 46.7991, l0_loss: 89.0516, frac_dead: 0.0000: 100%|██████████| 2/2 [00:00<00:00, 10.90it/s]\n",
      "l2_loss: 34.6084, l1_loss: 15.5941, l0_loss: 36.1923, frac_dead: 0.0000: 100%|██████████| 4/4 [00:00<00:00, 10.64it/s]\n",
      "l2_loss: 49.3018, l1_loss: 5.4314, l0_loss: 14.3419, frac_dead: 0.0020: 100%|██████████| 8/8 [00:00<00:00, 10.63it/s] \n",
      "l2_loss: 54.0092, l1_loss: 2.7360, l0_loss: 7.7546, frac_dead: 0.0000: 100%|██████████| 16/16 [00:01<00:00, 10.40it/s]\n",
      "l2_loss: 42.2358, l1_loss: 3.0398, l0_loss: 6.2715, frac_dead: 0.0000: 100%|██████████| 32/32 [00:03<00:00, 10.34it/s]\n",
      "l2_loss: 27.9374, l1_loss: 2.5359, l0_loss: 4.0195, frac_dead: 0.0000: 100%|██████████| 64/64 [00:07<00:00,  8.88it/s]\n",
      "l2_loss: 15.8682, l1_loss: 2.8819, l0_loss: 2.1441, frac_dead: 0.3379: 100%|██████████| 372/372 [00:34<00:00, 10.92it/s]\n",
      "l2_loss: 15.5220, l1_loss: 2.9068, l0_loss: 2.9355, frac_dead: 0.0566: 100%|██████████| 250/250 [00:22<00:00, 11.25it/s] \n",
      "l2_loss: 15.4141, l1_loss: 2.9150, l0_loss: 2.6935, frac_dead: 0.0859: 100%|██████████| 250/250 [00:21<00:00, 11.60it/s]\n",
      "l2_loss: 15.3461, l1_loss: 2.9222, l0_loss: 2.7338, frac_dead: 0.0195: 100%|██████████| 250/250 [00:22<00:00, 11.24it/s]\n",
      "l2_loss: 15.3304, l1_loss: 2.9229, l0_loss: 2.5751, frac_dead: 0.0352: 100%|██████████| 250/250 [00:22<00:00, 11.08it/s]\n",
      "l2_loss: 15.2649, l1_loss: 2.9326, l0_loss: 2.5292, frac_dead: 0.0371: 100%|██████████| 500/500 [00:45<00:00, 11.03it/s]\n",
      "l2_loss: 421.3169, l1_loss: 155.2439, l0_loss: 211.4837, frac_dead: 0.0059: 100%|██████████| 1/1 [00:00<00:00, 10.69it/s]\n",
      "l2_loss: 116.9926, l1_loss: 93.3447, l0_loss: 147.5790, frac_dead: 0.0117: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "l2_loss: 22.8204, l1_loss: 38.5614, l0_loss: 77.4992, frac_dead: 0.0254: 100%|██████████| 2/2 [00:00<00:00, 10.73it/s]\n",
      "l2_loss: 34.0056, l1_loss: 11.0039, l0_loss: 29.9082, frac_dead: 0.0625: 100%|██████████| 4/4 [00:00<00:00, 12.05it/s]\n",
      "l2_loss: 48.2408, l1_loss: 4.1425, l0_loss: 12.7170, frac_dead: 0.0996: 100%|██████████| 8/8 [00:00<00:00, 11.22it/s]\n",
      "l2_loss: 39.8688, l1_loss: 3.7740, l0_loss: 7.4187, frac_dead: 0.1113: 100%|██████████| 16/16 [00:01<00:00, 10.43it/s]\n",
      "l2_loss: 37.2759, l1_loss: 3.0487, l0_loss: 4.4368, frac_dead: 0.1289: 100%|██████████| 32/32 [00:03<00:00,  9.55it/s]\n",
      "l2_loss: 30.2450, l1_loss: 1.4965, l0_loss: 3.7437, frac_dead: 0.0918: 100%|██████████| 64/64 [00:07<00:00,  8.93it/s]\n",
      "l2_loss: 12.5951, l1_loss: 1.0099, l0_loss: 0.6811, frac_dead: 0.6992: 100%|██████████| 372/372 [00:35<00:00, 10.50it/s]\n",
      "l2_loss: 12.2210, l1_loss: 1.1073, l0_loss: 2.3918, frac_dead: 0.0996: 100%|██████████| 250/250 [00:24<00:00, 10.34it/s]    \n",
      "l2_loss: 11.6963, l1_loss: 1.1250, l0_loss: 1.8175, frac_dead: 0.2363: 100%|██████████| 250/250 [00:24<00:00, 10.19it/s]\n",
      "l2_loss: 11.5474, l1_loss: 1.1493, l0_loss: 1.5530, frac_dead: 0.3438: 100%|██████████| 250/250 [00:23<00:00, 10.49it/s] \n",
      "l2_loss: 11.5082, l1_loss: 1.1505, l0_loss: 1.3108, frac_dead: 0.3398: 100%|██████████| 250/250 [00:25<00:00,  9.80it/s]\n",
      "l2_loss: 11.4931, l1_loss: 1.1528, l0_loss: 1.2804, frac_dead: 0.3516: 100%|██████████| 500/500 [00:48<00:00, 10.23it/s]\n",
      "l2_loss: 373.8607, l1_loss: 151.3280, l0_loss: 209.3314, frac_dead: 0.0293: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "l2_loss: 60.0208, l1_loss: 77.4046, l0_loss: 135.9713, frac_dead: 0.0508: 100%|██████████| 1/1 [00:00<00:00, 12.53it/s]\n",
      "l2_loss: 17.4277, l1_loss: 23.8446, l0_loss: 62.3028, frac_dead: 0.0977: 100%|██████████| 2/2 [00:00<00:00, 12.56it/s]\n",
      "l2_loss: 40.4810, l1_loss: 5.6884, l0_loss: 20.8342, frac_dead: 0.1445: 100%|██████████| 4/4 [00:00<00:00, 11.56it/s]\n",
      "l2_loss: 40.9729, l1_loss: 3.4163, l0_loss: 9.3640, frac_dead: 0.1777: 100%|██████████| 8/8 [00:00<00:00, 11.51it/s] \n",
      "l2_loss: 22.9146, l1_loss: 4.2623, l0_loss: 4.9332, frac_dead: 0.2051: 100%|██████████| 16/16 [00:01<00:00, 12.07it/s]\n",
      "l2_loss: 17.2090, l1_loss: 4.0887, l0_loss: 2.2033, frac_dead: 0.2266: 100%|██████████| 32/32 [00:03<00:00,  9.93it/s]\n",
      "l2_loss: 17.0416, l1_loss: 2.0568, l0_loss: 2.1014, frac_dead: 0.1836: 100%|██████████| 64/64 [00:06<00:00,  9.98it/s]\n",
      "l2_loss: 15.6766, l1_loss: 0.2477, l0_loss: 0.9992, frac_dead: 0.9199: 100%|██████████| 372/372 [00:37<00:00,  9.92it/s]\n",
      "l2_loss: 15.0082, l1_loss: 0.3632, l0_loss: 2.3823, frac_dead: 0.5000: 100%|██████████| 250/250 [00:30<00:00,  8.20it/s]  \n",
      "l2_loss: 15.2350, l1_loss: 0.3250, l0_loss: 1.3494, frac_dead: 0.8340: 100%|██████████| 250/250 [00:25<00:00,  9.88it/s]\n",
      "l2_loss: 15.0306, l1_loss: 0.3629, l0_loss: 2.5080, frac_dead: 0.0332:  71%|███████   | 178/250 [00:19<00:07, 10.27it/s]  "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "with storage:\n",
    "    metrics_dfs = []\n",
    "    for node in NODES:\n",
    "        A_train = A_TRAIN_DICT[node]\n",
    "        A_train_normalized, scale = normalize_activations(A=A_train)\n",
    "\n",
    "        for l1_coeff in (DefaultConfig.L1_COEFF, ):\n",
    "            for lr in (DefaultConfig.LR, ):\n",
    "                for batch_size in (512, ):\n",
    "                    for dict_mult in (8, ):\n",
    "                        encoder_state_dict = None\n",
    "                        optimizer_state_dict = None\n",
    "                        scheduler_state_dict = None\n",
    "                        metrics_list = []\n",
    "                        d_hidden = dict_mult * 64\n",
    "                        for start_epoch, end_epoch in zip(CHECKPOINT_STEPS, CHECKPOINT_STEPS[1:]):\n",
    "                            encoder_state_dict, optimizer_state_dict, scheduler_state_dict, metrics = train_vanilla(\n",
    "                                A=A_train_normalized,\n",
    "                                start_epoch=start_epoch,\n",
    "                                d_hidden=d_hidden,\n",
    "                                end_epoch=end_epoch,\n",
    "                                batch_size=batch_size,\n",
    "                                encoder_state_dict=encoder_state_dict,\n",
    "                                optimizer_state_dict=optimizer_state_dict,\n",
    "                                scheduler_state_dict=scheduler_state_dict,\n",
    "                                l1_coeff=l1_coeff,\n",
    "                                lr=lr,\n",
    "                                resample_epochs=RESAMPLE_EPOCHS,\n",
    "                                final_decay_start=FINAL_DECAY_START,\n",
    "                                final_decay_end=FINAL_DECAY_END,\n",
    "                            )\n",
    "                            metrics_list.append(metrics)\n",
    "                            all_metrics = [elt for x in metrics_list for elt in storage.unwrap(x)]\n",
    "                            metrics_df = pd.DataFrame(all_metrics)\n",
    "                            metrics_df['l1_coeff'] = l1_coeff\n",
    "                            metrics_df['lr'] = lr\n",
    "                            metrics_df['dict_mult'] = dict_mult\n",
    "                            metrics_df['node'] = node.displayname\n",
    "                            metrics_df['batch_size'] = batch_size\n",
    "                            metrics_dfs.append(metrics_df)\n",
    "\n",
    "    metrics_df = pd.concat(metrics_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:34:03] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Dropped <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> calls <span style=\"font-weight: bold\">(</span>and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> from cache<span style=\"font-weight: bold\">)</span>.                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">storage.py:228</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[15:34:03]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Dropped \u001b[1;36m8\u001b[0m calls \u001b[1m(\u001b[0mand \u001b[1;36m8\u001b[0m from cache\u001b[1m)\u001b[0m.                                              \u001b[2mstorage.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m228\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Cleaning up <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">43</span> orphaned refs.                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">storage.py:159</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Cleaning up \u001b[1;36m43\u001b[0m orphaned refs.                                                    \u001b[2mstorage.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m159\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:34:04] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Cleaning up <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">43</span> unreferenced cids.                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">storage.py:164</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[15:34:04]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Cleaning up \u001b[1;36m43\u001b[0m unreferenced cids.                                                \u001b[2mstorage.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m164\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf = storage.cf(train_vanilla)\n",
    "cf.delete_calls()\n",
    "storage.cleanup_refs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(metrics_df).mark_line().encode(\n",
    "    x='epoch',\n",
    "    y='l0_loss',\n",
    "    color='batch_size:N',\n",
    ").interactive().properties(width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated SAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic April update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "# import cosine scheduler \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "checkpoint_steps = [0, 100, 200]\n",
    "\n",
    "encoder = VanillaAutoEncoder(d_activation=100, dict_mult=10).cuda()\n",
    "optim = Adam(encoder.parameters())\n",
    "scheduler = MidTrainingWarmupScheduler(optimizer=optim, num_warmup_steps=100, last_epoch=-1)\n",
    "encoder_state_dict = encoder.state_dict()\n",
    "optimizer_state_dict = optim.state_dict()\n",
    "scheduler_state_dict = scheduler.state_dict()\n",
    "for start_epoch, end_epoch in zip(checkpoint_steps, checkpoint_steps[1:]):\n",
    "    encoder_state_dict, optimizer_state_dict, scheduler_state_dict, metrics = train_vanilla(\n",
    "        A=torch.randn(100, 100).cuda(),\n",
    "        d_activation=100,\n",
    "        dict_mult=10,\n",
    "        start_epoch=start_epoch,\n",
    "        end_epoch=end_epoch,\n",
    "        encoder_state_dict=encoder_state_dict,\n",
    "        optimizer_state_dict=optimizer_state_dict,\n",
    "        scheduler_state_dict=scheduler_state_dict,\n",
    "        batch_size=10,\n",
    "        l1_coeff=1.0,\n",
    "        resample_every=50,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "55 * 20 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
