{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ioi_utils import *\n",
    "from sae_variants import *\n",
    "from training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 32, 32)\n",
    "a = nn.Parameter(x)\n",
    "x.requires_grad\n",
    "b = nn.Parameter(x.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_original = a.clone().detach()\n",
    "loss = b.sum()\n",
    "loss.backward()\n",
    "a_new = a.clone().detach()\n",
    "a_new - a_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = GatedAutoEncoder(d_activation=100, d_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "# import cosine scheduler \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "checkpoint_steps = [0, 100, 200]\n",
    "\n",
    "encoder = VanillaAutoEncoder(d_activation=100, dict_mult=10).cuda()\n",
    "optim = Adam(encoder.parameters())\n",
    "scheduler = MidTrainingWarmupScheduler(optimizer=optim, num_warmup_steps=100, last_epoch=-1)\n",
    "encoder_state_dict = encoder.state_dict()\n",
    "optimizer_state_dict = optim.state_dict()\n",
    "scheduler_state_dict = scheduler.state_dict()\n",
    "for start_epoch, end_epoch in zip(checkpoint_steps, checkpoint_steps[1:]):\n",
    "    encoder_state_dict, optimizer_state_dict, scheduler_state_dict, metrics = train_vanilla(\n",
    "        A=torch.randn(100, 100).cuda(),\n",
    "        d_activation=100,\n",
    "        dict_mult=10,\n",
    "        start_epoch=start_epoch,\n",
    "        end_epoch=end_epoch,\n",
    "        encoder_state_dict=encoder_state_dict,\n",
    "        optimizer_state_dict=optimizer_state_dict,\n",
    "        scheduler_state_dict=scheduler_state_dict,\n",
    "        batch_size=10,\n",
    "        l1_coeff=1.0,\n",
    "        resample_every=50,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
