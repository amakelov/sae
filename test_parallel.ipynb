{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ioi_utils import *\n",
    "from sae_variants import *\n",
    "from training import *\n",
    "from mandala._next.imports import *\n",
    "from mandala._next.common_imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circuit setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not find model\n"
     ]
    }
   ],
   "source": [
    "from circuit_utils import *\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "if 'model' in locals():\n",
    "    MODELS[MODEL_ID] = model\n",
    "\n",
    "HEAD_CLASS_FIG = {\n",
    "    'nm': 'Name Mover',\n",
    "    'bnm': 'Backup Name Mover',\n",
    "    'ind': 'Induction',\n",
    "    'nnm': 'Negative Name Mover',\n",
    "    'si': 'S-Inhibition',\n",
    "    'dt': 'Duplicate Token',\n",
    "    'pt': 'Previous Token',\n",
    "}\n",
    "\n",
    "COMPONENT_NAME_FIG = {\n",
    "    'k': 'Key',\n",
    "    'v': 'Value',\n",
    "    'q': 'Query',\n",
    "    'z': 'Attn Output',\n",
    "}\n",
    "\n",
    "CROSS_SECTION_FIG = {\n",
    "    'ind+dt@z': 'Ind+DT out',\n",
    "    'nm+bnm@q': '(B)NM q',\n",
    "    'nm+bnm@qk': '(B)NM qk',\n",
    "    'nm+bnm@z': '(B)NM out',\n",
    "    'si@v': 'S-I v',\n",
    "    'si@z': 'S-I out',\n",
    "}\n",
    "\n",
    "c = Circuit()\n",
    "paper_cross_sections = [\n",
    "    # IO\n",
    "    (c.zs(c.nm + c.bnm), ('io',), 'nm+bnm@z'),\n",
    "    (c.qs(c.nm + c.bnm) + c.ks(c.nm + c.bnm), ('io',), 'nm+bnm@qk'),\n",
    "    (c.qs(c.nm + c.bnm), ('io',), 'nm+bnm@q'),\n",
    "    # S\n",
    "    (c.qs(c.nm + c.bnm) + c.ks(c.nm + c.bnm), ('s',), 'nm+bnm@qk'),\n",
    "    (c.qs(c.nm + c.bnm), ('s',), 'nm+bnm@q'),\n",
    "    (c.vs(c.si), ('s',), 'si@v'),\n",
    "    (c.zs(c.si), ('s',), 'si@z'),\n",
    "    (c.zs(c.ind) + c.zs(c.dt), ('s',), 'ind+dt@z'),\n",
    "    # Pos\n",
    "    (c.qs(c.nm + c.bnm) + c.ks(c.nm + c.bnm), ('io_pos',), 'nm+bnm@qk'),\n",
    "    (c.qs(c.nm + c.bnm), ('io_pos',), 'nm+bnm@q'),\n",
    "    (c.zs(c.si), ('io_pos',), 'si@z'),\n",
    "    (c.vs(c.si), ('io_pos',), 'si@v'),\n",
    "    # Pos + S\n",
    "    (c.qs(c.nm + c.bnm) + c.ks(c.nm + c.bnm), ('io_pos', 's'), 'nm+bnm@qk'),\n",
    "    (c.zs(c.si), ('io_pos', 's'), 'si@z'),\n",
    "    (c.vs(c.si), ('io_pos', 's'), 'si@v'),\n",
    "    (c.zs(c.ind) + c.zs(c.dt), ('io_pos', 's'), 'ind+dt@z'),\n",
    "    (c.zs(c.ind) + c.zs(c.dt), ('io_pos',), 'ind+dt@z'),\n",
    "    # All\n",
    "    (c.qs(c.nm + c.bnm) + c.ks(c.nm + c.bnm), ('io', 'io_pos', 's'), 'nm+bnm@qk'),\n",
    "]\n",
    "\n",
    "locations_displaynames = {\n",
    "    'nm+bnm@z': '(B)NM out',\n",
    "    'nm+bnm@qk': '(B)NM qk',\n",
    "    'nm+bnm@q': '(B)NM q',\n",
    "    'si@v': 'S-I v',\n",
    "    'si@z': 'S-I out',\n",
    "    'ind+dt@z': 'Ind+DT out',\n",
    "}\n",
    "\n",
    "NODES = c.zs(c.nm + c.bnm) + c.qs(c.nm + c.bnm) + c.zs(c.si) + [n for n in c.vs(c.si) if n.seq_pos == 's2'] + c.zs(c.ind) + c.zs(c.dt) + c.ks(c.nm + c.bnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = '/media/amakelov/SanDisk1TB/paper_sprint/test.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage(db_path=DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amakelov/workspace/current/conda_envs/serimats/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "MODELS[MODEL_ID] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logging level to debug\n",
    "from mandala._next.common_imports import logger\n",
    "import logging\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 17.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# nodes = list(c.nodes.keys())\n",
    "# circuit_nodes = list(c.nodes.keys())\n",
    "\n",
    "with storage:\n",
    "\n",
    "    ############################################################################ \n",
    "    ### prompt dataset for training supervised features\n",
    "    ############################################################################ \n",
    "    P_train = generate_prompts(\n",
    "        distribution=full_distribution,\n",
    "        patterns=['ABB', 'BAB'],\n",
    "        prompts_per_pattern=10_000,\n",
    "        random_seed=0,\n",
    "    )\n",
    "    N_TRAIN = len(storage.unwrap(P_train))\n",
    "    ### activations for training supervised features\n",
    "    As_train = run_with_cache(\n",
    "        prompts=P_train, \n",
    "        nodes=NODES,\n",
    "        batch_size=100,\n",
    "        model_id=MODEL_ID,\n",
    "        verbose=True,\n",
    "    )\n",
    "    A_TRAIN_DICT = {node: A for node, A in zip(NODES, As_train)}\n",
    "\n",
    "    ### precompute the mean logit difference for clean training data\n",
    "    logits_train_clean = run_with_hooks(prompts=P_train, hooks=[], batch_size=200,)\n",
    "    CLEAN_LD_MEAN = (storage.unwrap(logits_train_clean)[:, 0] - storage.unwrap(logits_train_clean)[:, 1]).mean().item()\n",
    "\n",
    "    ### precompute the mean-ablated logit difference when ablating each node\n",
    "    A_TRAIN_MEAN_DICT = {node: get_dataset_mean(A) for node, A in A_TRAIN_DICT.items()}\n",
    "\n",
    "    MEAN_ABLATED_LD_DICT = {}\n",
    "    for node, A in A_TRAIN_DICT.items():\n",
    "        MEAN_ABLATED_LD_DICT[node] = compute_mean_ablated_lds(\n",
    "            node=node, prompts=P_train, A_mean=A_TRAIN_MEAN_DICT[node], batch_size=200,\n",
    "        )\n",
    "\n",
    "    ############################################################################ \n",
    "    ### prompt dataset for editing\n",
    "    ############################################################################ \n",
    "    N_NAMES = len(NAMES)\n",
    "    editing_base_distribution = copy.deepcopy(full_distribution)\n",
    "    editing_base_distribution.names = editing_base_distribution.names[:N_NAMES // 2]\n",
    "    editing_source_distribution = copy.deepcopy(full_distribution)\n",
    "    editing_source_distribution.names = editing_source_distribution.names[N_NAMES // 2:]\n",
    "\n",
    "    P_edit = generate_prompts(\n",
    "        distribution=editing_base_distribution,\n",
    "        patterns=['ABB', 'BAB'],\n",
    "        prompts_per_pattern=2500,\n",
    "        random_seed=1,\n",
    "    )\n",
    "    As_to_edit = run_with_cache(\n",
    "        prompts=P_edit, \n",
    "        nodes=NODES,\n",
    "        batch_size=100,\n",
    "        model_id=MODEL_ID,\n",
    "        verbose=True,\n",
    "    )\n",
    "    A_EDIT_DICT = {node: A for node, A in zip(NODES, As_to_edit)}\n",
    "\n",
    "    N_EDIT = len(storage.unwrap(P_edit))\n",
    "    N_NAMES_EDIT_SOURCE = len(editing_source_distribution.names)\n",
    "\n",
    "    ############################################################################ \n",
    "    ### Compute counterfactual prompts and activations\n",
    "    ############################################################################ \n",
    "    FEATURE_SUBSETS = [('io_pos',), ('s',), ('io',), ] # ('s', 'io_pos',), ('io', 'io_pos'), ('s', 'io',), ('io_pos', 's', 'io',), ]\n",
    "\n",
    "    CF_PROMPTS_DICT = {}\n",
    "    for feature_subset in FEATURE_SUBSETS:\n",
    "        CF_PROMPTS_DICT[feature_subset] = get_cf_prompts(\n",
    "            prompts=P_edit, \n",
    "            features=feature_subset,\n",
    "            io_targets=generate_name_samples(N_EDIT, editing_source_distribution.names[:N_NAMES_EDIT_SOURCE // 2]),\n",
    "            s_targets=generate_name_samples(N_EDIT, editing_source_distribution.names[N_NAMES_EDIT_SOURCE//2:]),     \n",
    "        )\n",
    "    ### Compute counterfactual activations\n",
    "    As_counterfactual = {}\n",
    "    for feature_subset, cf_prompts in tqdm(CF_PROMPTS_DICT.items()):\n",
    "        As_counterfactual[feature_subset] = run_with_cache(\n",
    "            prompts=cf_prompts, \n",
    "            nodes=NODES,\n",
    "            batch_size=100,\n",
    "            model_id=MODEL_ID,\n",
    "            verbose=True,\n",
    "        )\n",
    "    for feature_subset in As_counterfactual:\n",
    "        As_counterfactual[feature_subset] = {node: As_counterfactual[feature_subset][i] for i, node in enumerate(NODES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.cache_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing supervised features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage:\n",
    "    SUPERVISED_FEATURES_DICT = {}\n",
    "    SUPERVISED_RECONSTRUCTIONS_DICT = {}\n",
    "    for node, A in tqdm(A_TRAIN_DICT.items()):\n",
    "        for eventually in ['independent',]: # 'coupled', 'names', ]:\n",
    "            for codes_type in ('mean',):  # 'mse'):\n",
    "                node_parametrization = get_parametrization(node=node, eventually=eventually, use_names= (eventually == 'names'))\n",
    "                node_features = FEATURE_CONFIGURATIONS[node_parametrization]\n",
    "                code_getter = get_mean_codes if codes_type == 'mean' else lambda features, A, prompts: train_mse_codes(features=features, A=A, prompts=prompts, manual_bias=True)\n",
    "                codes, reconstructions = code_getter(\n",
    "                    features=node_features,\n",
    "                    A=A,\n",
    "                    prompts=P_train,\n",
    "                )\n",
    "                SUPERVISED_FEATURES_DICT[(node, node_parametrization, codes_type)] = codes\n",
    "                SUPERVISED_RECONSTRUCTIONS_DICT[(node, node_parametrization, codes_type)] = reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define a uniform schedule for all training runs\n",
    "\n",
    "# use exponentially spread-out checkpoints for the very early stages of training\n",
    "# measure right before resampling, as well as in the middle between resamplings\n",
    "# measure before and after the final LR decay\n",
    "# use two resampling stages, as it seems effects diminish after the first one\n",
    "CHECKPOINT_STEPS = [0, 1, 2, 4, 8, 16, 32, 64, 128, 500, 750, 1000, 1250, 1500, 2000]\n",
    "RESAMPLE_EPOCHS = [501, 1001, ]\n",
    "FINAL_DECAY_START = 1500 # decay the LR for the last 25% of training\n",
    "FINAL_DECAY_END = 2000\n",
    "\n",
    "### define hyperparam grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "with storage:\n",
    "    metrics_dfs = []\n",
    "    for node in NODES:\n",
    "        A_train = A_TRAIN_DICT[node]\n",
    "        A_train_normalized, scale = normalize_activations(A=A_train)\n",
    "\n",
    "        for l1_coeff in (DefaultConfig.L1_COEFF, ):\n",
    "            for lr in (DefaultConfig.LR, ):\n",
    "                for batch_size in (512, ):\n",
    "                    for dict_mult in (8, ):\n",
    "                        encoder_state_dict = None\n",
    "                        optimizer_state_dict = None\n",
    "                        scheduler_state_dict = None\n",
    "                        metrics_list = []\n",
    "                        d_hidden = dict_mult * 64\n",
    "                        for start_epoch, end_epoch in zip(CHECKPOINT_STEPS, CHECKPOINT_STEPS[1:]):\n",
    "                            encoder_state_dict, optimizer_state_dict, scheduler_state_dict, metrics = train_vanilla(\n",
    "                                A=A_train_normalized,\n",
    "                                start_epoch=start_epoch,\n",
    "                                d_hidden=d_hidden,\n",
    "                                end_epoch=end_epoch,\n",
    "                                batch_size=batch_size,\n",
    "                                encoder_state_dict=encoder_state_dict,\n",
    "                                optimizer_state_dict=optimizer_state_dict,\n",
    "                                scheduler_state_dict=scheduler_state_dict,\n",
    "                                l1_coeff=l1_coeff,\n",
    "                                lr=lr,\n",
    "                                resample_epochs=RESAMPLE_EPOCHS,\n",
    "                                final_decay_start=FINAL_DECAY_START,\n",
    "                                final_decay_end=FINAL_DECAY_END,\n",
    "                            )\n",
    "                            metrics_list.append(metrics)\n",
    "                            all_metrics = [elt for x in metrics_list for elt in storage.unwrap(x)]\n",
    "                            metrics_df = pd.DataFrame(all_metrics)\n",
    "                            metrics_df['l1_coeff'] = l1_coeff\n",
    "                            metrics_df['lr'] = lr\n",
    "                            metrics_df['dict_mult'] = dict_mult\n",
    "                            metrics_df['node'] = node.displayname\n",
    "                            metrics_df['batch_size'] = batch_size\n",
    "                            metrics_dfs.append(metrics_df)\n",
    "\n",
    "    metrics_df = pd.concat(metrics_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = storage.cf(train_vanilla)\n",
    "cf.delete_calls()\n",
    "storage.cleanup_refs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(metrics_df).mark_line().encode(\n",
    "    x='epoch',\n",
    "    y='l0_loss',\n",
    "    color='batch_size:N',\n",
    ").interactive().properties(width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.preload_calls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage:\n",
    "    metrics_dfs = []\n",
    "    for node in tqdm(NODES):\n",
    "        A_train = A_TRAIN_DICT[node]\n",
    "        A_train_normalized, scale = normalize_activations(A=A_train)\n",
    "\n",
    "        for l1_coeff in (DefaultConfig.L1_COEFF, 1.0, 2.5):\n",
    "            for lr in (DefaultConfig.LR, ):\n",
    "                for batch_size in (512, ):\n",
    "                    for dict_mult in (8, ):\n",
    "                        encoder_state_dict = None\n",
    "                        optimizer_state_dict = None\n",
    "                        scheduler_state_dict = None\n",
    "                        metrics_list = []\n",
    "                        d_hidden = dict_mult * 64\n",
    "                        for start_epoch, end_epoch in zip(CHECKPOINT_STEPS, CHECKPOINT_STEPS[1:]):\n",
    "                            encoder_state_dict, optimizer_state_dict, scheduler_state_dict, metrics = train_gated(\n",
    "                                A=A_train_normalized,\n",
    "                                start_epoch=start_epoch,\n",
    "                                d_hidden=d_hidden,\n",
    "                                end_epoch=end_epoch,\n",
    "                                batch_size=batch_size,\n",
    "                                encoder_state_dict=encoder_state_dict,\n",
    "                                optimizer_state_dict=optimizer_state_dict,\n",
    "                                scheduler_state_dict=scheduler_state_dict,\n",
    "                                l1_coeff=l1_coeff,\n",
    "                                lr=lr,\n",
    "                                resample_epochs=RESAMPLE_EPOCHS,\n",
    "                                final_decay_start=FINAL_DECAY_START,\n",
    "                                final_decay_end=FINAL_DECAY_END,\n",
    "                            )\n",
    "                            metrics_list.append(metrics)\n",
    "                            all_metrics = [elt for x in metrics_list for elt in storage.unwrap(x)]\n",
    "                            metrics_df = pd.DataFrame(all_metrics)\n",
    "                            metrics_df['l1_coeff'] = l1_coeff\n",
    "                            metrics_df['lr'] = lr\n",
    "                            metrics_df['dict_mult'] = dict_mult\n",
    "                            metrics_df['node'] = node.displayname\n",
    "                            metrics_df['batch_size'] = batch_size\n",
    "                            metrics_dfs.append(metrics_df)\n",
    "        storage.commit()\n",
    "\n",
    "    metrics_df = pd.concat(metrics_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(metrics_df.query('1100 < epoch < 2000 and l1_coeff == 5.0')).mark_line().encode(\n",
    "    x='epoch',\n",
    "    y='l0_loss',\n",
    "    color='node:N',\n",
    "    strokeDash='l1_coeff:N',\n",
    ").properties(width=800, height=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = storage.cf(train_gated)\n",
    "cf.delete_calls()\n",
    "storage.cleanup_refs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution SAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_train_raw = storage.unwrap(P_train)[:40]\n",
    "\n",
    "grads = collect_gradients(\n",
    "    prompts=P_train_raw,\n",
    "    layers_and_activations=[\n",
    "        (0, 'z'), (2, 'z'), (3, 'z'), (4, 'z'), (5, 'z'), (6, 'z'), (7, 'z'), (8, 'z'), (9, 'z'), (10, 'z'), (11, 'z'),\n",
    "        (9, 'k',), (10, 'k'), (11, 'k'),\n",
    "        (9, 'q',), (10, 'q'), (11, 'q'),\n",
    "    ],\n",
    "    batch_size=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mandala._next.utils import serialize\n",
    "x = serialize(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/100\n",
      "Batch 1/100\n",
      "Batch 2/100\n",
      "Batch 3/100\n",
      "Batch 4/100\n",
      "Batch 5/100\n",
      "Batch 6/100\n",
      "Batch 7/100\n",
      "Batch 8/100\n",
      "Batch 9/100\n",
      "Batch 10/100\n",
      "Batch 11/100\n",
      "Batch 12/100\n",
      "Batch 13/100\n",
      "Batch 14/100\n",
      "Batch 15/100\n",
      "Batch 16/100\n",
      "Batch 17/100\n",
      "Batch 18/100\n",
      "Batch 19/100\n",
      "Batch 20/100\n",
      "Batch 21/100\n",
      "Batch 22/100\n",
      "Batch 23/100\n",
      "Batch 24/100\n",
      "Batch 25/100\n",
      "Batch 26/100\n",
      "Batch 27/100\n",
      "Batch 28/100\n",
      "Batch 29/100\n",
      "Batch 30/100\n",
      "Batch 31/100\n",
      "Batch 32/100\n",
      "Batch 33/100\n",
      "Batch 34/100\n",
      "Batch 35/100\n",
      "Batch 36/100\n",
      "Batch 37/100\n",
      "Batch 38/100\n",
      "Batch 39/100\n",
      "Batch 40/100\n",
      "Batch 41/100\n",
      "Batch 42/100\n",
      "Batch 43/100\n",
      "Batch 44/100\n",
      "Batch 45/100\n",
      "Batch 46/100\n",
      "Batch 47/100\n",
      "Batch 48/100\n",
      "Batch 49/100\n",
      "Batch 50/100\n",
      "Batch 51/100\n",
      "Batch 52/100\n",
      "Batch 53/100\n",
      "Batch 54/100\n",
      "Batch 55/100\n",
      "Batch 56/100\n",
      "Batch 57/100\n",
      "Batch 58/100\n",
      "Batch 59/100\n",
      "Batch 60/100\n",
      "Batch 61/100\n",
      "Batch 62/100\n",
      "Batch 63/100\n",
      "Batch 64/100\n",
      "Batch 65/100\n",
      "Batch 66/100\n",
      "Batch 67/100\n",
      "Batch 68/100\n",
      "Batch 69/100\n",
      "Batch 70/100\n",
      "Batch 71/100\n",
      "Batch 72/100\n",
      "Batch 73/100\n",
      "Batch 74/100\n",
      "Batch 75/100\n",
      "Batch 76/100\n",
      "Batch 77/100\n",
      "Batch 78/100\n",
      "Batch 79/100\n",
      "Batch 80/100\n",
      "Batch 81/100\n",
      "Batch 82/100\n",
      "Batch 83/100\n",
      "Batch 84/100\n",
      "Batch 85/100\n",
      "Batch 86/100\n",
      "Batch 87/100\n",
      "Batch 88/100\n",
      "Batch 89/100\n",
      "Batch 90/100\n",
      "Batch 91/100\n",
      "Batch 92/100\n",
      "Batch 93/100\n",
      "Batch 94/100\n",
      "Batch 95/100\n",
      "Batch 96/100\n",
      "Batch 97/100\n",
      "Batch 98/100\n",
      "Batch 99/100\n"
     ]
    }
   ],
   "source": [
    "with storage:\n",
    "    COMPUTING = False\n",
    "    P_train_raw = storage.unwrap(P_train)\n",
    "    n_total = len(P_train_raw)\n",
    "    n_batches = 100\n",
    "    grads_parts = []\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        print(f'Batch {i}/{n_batches}')\n",
    "        start = i * (n_total // n_batches)\n",
    "        end = (i + 1) * (n_total // n_batches)\n",
    "        prompts = P_train_raw[start:end]\n",
    "        grads = collect_gradients(\n",
    "            prompts=prompts,\n",
    "            nodes=NODES,\n",
    "            batch_size=20,\n",
    "        )\n",
    "        if COMPUTING:\n",
    "            storage.commit()\n",
    "            storage.atoms.clear()\n",
    "        grads_parts.append(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.unwrap(grads)[NODES[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = storage.cf(collect_gradients)\n",
    "cf.delete_calls()\n",
    "storage.cleanup_refs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    print(storage.unwrap(grads)[(10, 'z')][:, -1, i, :].norm(dim=-1).mean(), storage.unwrap(grads)[(10, 'z')][:, 10, :, :].norm(dim=-1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic April update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "# import cosine scheduler \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "checkpoint_steps = [0, 100, 200]\n",
    "\n",
    "encoder = VanillaAutoEncoder(d_activation=100, dict_mult=10).cuda()\n",
    "optim = Adam(encoder.parameters())\n",
    "scheduler = MidTrainingWarmupScheduler(optimizer=optim, num_warmup_steps=100, last_epoch=-1)\n",
    "encoder_state_dict = encoder.state_dict()\n",
    "optimizer_state_dict = optim.state_dict()\n",
    "scheduler_state_dict = scheduler.state_dict()\n",
    "for start_epoch, end_epoch in zip(checkpoint_steps, checkpoint_steps[1:]):\n",
    "    encoder_state_dict, optimizer_state_dict, scheduler_state_dict, metrics = train_vanilla(\n",
    "        A=torch.randn(100, 100).cuda(),\n",
    "        d_activation=100,\n",
    "        dict_mult=10,\n",
    "        start_epoch=start_epoch,\n",
    "        end_epoch=end_epoch,\n",
    "        encoder_state_dict=encoder_state_dict,\n",
    "        optimizer_state_dict=optimizer_state_dict,\n",
    "        scheduler_state_dict=scheduler_state_dict,\n",
    "        batch_size=10,\n",
    "        l1_coeff=1.0,\n",
    "        resample_every=50,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "55 * 20 / 60"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
