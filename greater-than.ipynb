{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ioi_utils import Node\n",
    "from mandala._next.imports import *\n",
    "from mandala._next.common_imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A proof-of-concept evaluation for the greater-than circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reproduce the main aspects of the dataset of https://arxiv.org/pdf/2305.00586\n",
    "with open('data/gt-nouns.txt', 'r') as f:\n",
    "    NOUNS = f.read().split('\\n')\n",
    "\n",
    "# figure out which years are tokenized the way we want\n",
    "prompt = \"The war lasted from the year 1732 to the year 17\"\n",
    "\n",
    "HEAD_LOCATIONS = [(5, 1), (5, 5), (6, 1), (6, 9), (7, 10), (8, 8), (8, 11)]\n",
    "NODES = [Node(component_name='z', layer=layer, head=head, seq_pos=-1) for layer, head in HEAD_LOCATIONS]\n",
    "YYS = ['23', '42'] # TODO\n",
    "\n",
    "class Prompt:\n",
    "    def __init__(self,\n",
    "                 noun: str, \n",
    "                 yy: str,\n",
    "                 xx: str = '17', # for simplicity, everything is set in the 18th century\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        - xx: the century\n",
    "        - yy: the last two digits of the year\n",
    "        \"\"\"\n",
    "        self.noun = noun\n",
    "        self.xx = xx\n",
    "        self.yy = yy\n",
    "    \n",
    "    @property\n",
    "    def sentence(self) -> str:\n",
    "        return f\"The {self.noun} lasted from the year {self.xx}{self.yy} to the year {self.xx}\"\n",
    "    \n",
    "    def with_changed_yy(self, yy: str) -> 'Prompt':\n",
    "        return Prompt(noun=self.noun, xx=self.xx, yy=yy)\n",
    "\n",
    "class PromptDistribution:\n",
    "    \"\"\"\n",
    "    A class to represent a distribution over prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        yys: List[str],\n",
    "        nouns: List[str],\n",
    "    ):\n",
    "        self.yys = yys\n",
    "        self.nouns = nouns\n",
    "\n",
    "    def sample_one(self,) -> Prompt:\n",
    "        \"\"\"\n",
    "        Sample a single prompt from the distribution.\n",
    "        \"\"\"\n",
    "        yy = random.choice(self.yys)\n",
    "        noun = random.choice(self.nouns)\n",
    "        return Prompt(noun=noun, yy=yy)\n",
    "\n",
    "\n",
    "FULL_DISTRIBUTION = PromptDistribution(yys=YYS, nouns=NOUNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Sequence\n",
    "from torch import Tensor\n",
    "MODEL_ID = 'gpt2-small'\n",
    "\n",
    "\n",
    "FEATURE_SUBSETS = [\n",
    "    ('yy',),\n",
    "]\n",
    "FEATURE_SUBSETS = [tuple(sorted(x)) for x in FEATURE_SUBSETS]\n",
    "\n",
    "def get_yy_to_idx(distribution: PromptDistribution) -> Dict[str, int]:\n",
    "    # return {name: i for i, name in enumerate(distribution.names)}\n",
    "    return {yy: i for i, yy in distribution.yys}\n",
    "\n",
    "# setup to work with features\n",
    "YY_TO_IDX = get_yy_to_idx(FULL_DISTRIBUTION)\n",
    "\n",
    "FEATURE_SIZES = {\n",
    "    'bias': 1,\n",
    "    'yy': len(YY_TO_IDX),\n",
    "}\n",
    "\n",
    "# this collects possible ways to parametrize activations of the model\n",
    "FEATURE_CONFIGURATIONS = {\n",
    "    'independent': [('yy', ), ],\n",
    "}\n",
    "# add bias to each\n",
    "FEATURE_CONFIGURATIONS = {k: [('bias', )] + v for k, v in FEATURE_CONFIGURATIONS.items()}\n",
    "\n",
    "@op\n",
    "def generate_name_samples(n_samples, names, random_seed: int = 0) -> Any:\n",
    "    np.random.seed(random_seed)\n",
    "    return np.random.choice(names, n_samples, replace=True)\n",
    "\n",
    "@op\n",
    "def get_cf_prompts(\n",
    "    prompts: List[Prompt],\n",
    "    features: Tuple[str, ...],\n",
    "    yy_targets: List[str],\n",
    ") -> Any:\n",
    "    assert features == ('yy',)\n",
    "    return [p.with_changed_yy(yy=yy) for p, yy in zip(prompts, yy_targets)]\n",
    "\n",
    "@op\n",
    "def generate_prompts(distribution: PromptDistribution, random_seed: int, n_prompts: int,\n",
    "                     ) -> Any:\n",
    "    random.seed(random_seed)\n",
    "    return [distribution.sample_one() for _ in range(n_prompts)]\n",
    "\n",
    "################################################################################\n",
    "### working with features\n",
    "################################################################################\n",
    "def get_prompt_representation(p: Prompt) -> Dict[str, int]:\n",
    "    # extracts feature values from a prompt\n",
    "    return { 'yy': YY_TO_IDX[p.yy] }\n",
    "\n",
    "def get_prompt_feature_vals(p: Prompt) -> Dict[str, Any]:\n",
    "    return { 'yy': p.yy }\n",
    "\n",
    "def get_feature_shape(feature: Tuple[str,...]) -> Tuple[int, ...]:\n",
    "    return tuple(FEATURE_SIZES[f] for f in feature)\n",
    "\n",
    "def get_feature_deep_idx(feature: Tuple[str,...], prompt_rep: Dict[str, int]) -> Tuple[int,...]:\n",
    "    # given the feature values for a prompt, returns the index we should use to\n",
    "    # index into a code representing that feature (without the last dimension,\n",
    "    # which is the dimension of the code vectors and may vary)\n",
    "    return tuple([prompt_rep[f] for f in feature])\n",
    "\n",
    "@op\n",
    "def get_prompt_feature_idxs(prompts: Optional[Sequence[Prompt]],\n",
    "                            features: List[Tuple[str,...]],\n",
    "                            prompt_reps: Optional[List[dict]] = None,\n",
    "                            ) -> Dict[Tuple[str,...], Tensor]:\n",
    "    \"\"\"\n",
    "    Return a dictionary mapping each feature to a batch of indices into the\n",
    "    code for this feature over the prompts (indices don't take into account the\n",
    "    last dimension in the codes, which is the dimension of the code vectors and\n",
    "    may vary).\n",
    "    \"\"\"\n",
    "    if prompt_reps is None:\n",
    "        assert prompts is not None\n",
    "        prompt_reps = [get_prompt_representation(p) for p in prompts]\n",
    "    prompt_feature_idxs = {f: torch.tensor([get_feature_deep_idx(f, prompt_rep)\n",
    "                                            for prompt_rep in prompt_reps])\n",
    "                            for f in features}\n",
    "    return prompt_feature_idxs\n",
    "\n",
    "def get_reconstructions(\n",
    "    codes: Any, # Dict[tuple, Tensor],\n",
    "    prompt_feature_idxs: Optional[Dict[tuple, Tensor]] = None,\n",
    "    prompts: Optional[Sequence[Prompt]] = None,\n",
    "    decomposed: bool = False,\n",
    "    ) -> Union[Tensor, Dict[tuple, Tensor]]:\n",
    "    \"\"\"\n",
    "    Reconstruct prompts according to the given codes. if prompt_feature_idxs is\n",
    "    not given, it will be computed from prompts.\n",
    "    \"\"\"\n",
    "    prompt_vectors = {}\n",
    "    if prompt_feature_idxs is None:\n",
    "        assert prompts is not None\n",
    "        prompt_feature_idxs = get_prompt_feature_idxs(prompts, codes.keys())\n",
    "    for f, idx in prompt_feature_idxs.items():\n",
    "        # add the last dimension\n",
    "        full_idx = tuple([idx[:, i].cpu().numpy() for i in range(idx.shape[1])] + [slice(None, None, None)])\n",
    "        prompt_vectors[f] = codes[f][full_idx]\n",
    "    if not decomposed:\n",
    "        return sum(prompt_vectors.values())\n",
    "    else:\n",
    "        return prompt_vectors\n",
    "\n",
    "################################################################################\n",
    "### computing codes\n",
    "################################################################################\n",
    "@op\n",
    "def get_mean_codes(\n",
    "    features: List[Tuple[str,...]],\n",
    "    A: Tensor,\n",
    "    prompts: Any, # List[Prompt]\n",
    ") -> Tuple[Any, Any]:\n",
    "    \"\"\"\n",
    "    Compute codes using the mean of the (centered) activations for a given\n",
    "    feature value.\n",
    "    \"\"\"\n",
    "    # get the shape of the code for each feature\n",
    "    feature_shapes = {f: get_feature_shape(f) for f in features}\n",
    "    dim = A.shape[1]\n",
    "    feature_shapes = {f: tuple(list(feature_shapes[f]) + [dim]) for f in features}\n",
    "    # get the attributes of the prompts\n",
    "    prompt_feature_idxs = get_prompt_feature_idxs(prompts=prompts, features=features)\n",
    "    # group the prompt feature indices by feature value:\n",
    "    prompt_feature_groups = {\n",
    "        f: {} # will be {value in feature_idxs: [indices where this value appears in feature_idxs]}\n",
    "        for f in features\n",
    "    }\n",
    "    for f, feature_idxs in prompt_feature_idxs.items():\n",
    "        # populate prompt_feature_groups[f] = {value in feature_idxs: [indices\n",
    "        # where this value appears in feature_idxs]}\n",
    "        for idx, feature_idx in enumerate(feature_idxs):\n",
    "            value = tuple([x.item() for x in feature_idx])\n",
    "            if value not in prompt_feature_groups[f]:\n",
    "                prompt_feature_groups[f][value] = []\n",
    "            prompt_feature_groups[f][value].append(idx)\n",
    "        # convert to tensors\n",
    "        for value, indices in prompt_feature_groups[f].items():\n",
    "            prompt_feature_groups[f][value] = torch.tensor(indices)\n",
    "    codes = {\n",
    "        f: torch.zeros(feature_shape).cuda() for f, feature_shape in feature_shapes.items()\n",
    "    }\n",
    "    A_mean = A.mean(dim=0)\n",
    "    if ('bias',) in features:\n",
    "        codes[('bias',)] = A_mean.unsqueeze(0)\n",
    "    A_centered = A - A_mean\n",
    "    for f, groups in prompt_feature_groups.items():\n",
    "        if f != ('bias',):\n",
    "            for value, indices in groups.items():\n",
    "                codes[f][value] = A_centered[indices].mean(dim=0)\n",
    "    reconstructions = get_reconstructions(\n",
    "        codes=codes, prompt_feature_idxs=prompt_feature_idxs,\n",
    "    )\n",
    "    return codes, reconstructions\n",
    "\n",
    "\n",
    "################################################################################\n",
    "### feature editing\n",
    "################################################################################\n",
    "def get_edited_act(\n",
    "    val: Tensor,\n",
    "    method: str,\n",
    "    feature_idxs_to_delete: Dict[Tuple[str,...], List[Tuple[int,...]]],\n",
    "    feature_idxs_to_insert: Dict[Tuple[str,...], List[Tuple[int,...]]],\n",
    "    codes: Dict[Tuple[str,...], Tensor],\n",
    "    A_reference: Optional[Tensor] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    The core editing function: perform one or several edits on an activation\n",
    "    using the given codes and method.\n",
    "\n",
    "    Note that the methods based on subspace ablations are not commutative with\n",
    "    respect to the order of feature insertion/deletion. \n",
    "    \"\"\"\n",
    "    val = val.clone()\n",
    "    if method == 'arithmetic':\n",
    "        for f, idx_to_delete in feature_idxs_to_delete.items():\n",
    "            val = val - torch.stack([codes[f][i] for i in idx_to_delete])\n",
    "        for f, idx_to_insert in feature_idxs_to_insert.items():\n",
    "            val = val + torch.stack([codes[f][i] for i in idx_to_insert])\n",
    "    elif method == 'zero_ablate_subspace':\n",
    "        for f, idx_to_delete in feature_idxs_to_delete.items():\n",
    "            code_to_delete = torch.stack([codes[f][i] for i in idx_to_delete])\n",
    "            code_to_delete = code_to_delete / code_to_delete.norm(dim=-1, keepdim=True)\n",
    "            projections = einsum('batch dim, batch dim -> batch', val, code_to_delete)\n",
    "            val = val + einsum('batch, batch dim -> batch dim', - projections, code_to_delete)\n",
    "        for f, idx_to_insert in feature_idxs_to_insert.items():\n",
    "            val = val + torch.stack([codes[f][i] for i in idx_to_insert])\n",
    "    elif method == 'mean_ablate_subspace':\n",
    "        assert A_reference is not None\n",
    "        for f, idx_to_delete in feature_idxs_to_delete.items():\n",
    "            code_to_delete = torch.stack([codes[f][i] for i in idx_to_delete])\n",
    "            code_to_delete = code_to_delete / code_to_delete.norm(dim=-1, keepdim=True)\n",
    "            # mean_projection = (A_reference @ code_to_delete).mean(dim=0)\n",
    "            reference_projections = einsum('dim, batch dim -> batch', A_reference.mean(dim=0), code_to_delete)\n",
    "            projections = einsum('batch dim, batch dim -> batch', (reference_projections.unsqueeze(1) - val), code_to_delete)\n",
    "            val = val + einsum('batch, batch dim -> batch dim', projections, code_to_delete)\n",
    "        for f, idx_to_insert in feature_idxs_to_insert.items():\n",
    "            val = val + torch.stack([codes[f][i] for i in idx_to_insert])\n",
    "    else:\n",
    "        raise ValueError(f'unknown method {method}')\n",
    "    return val\n",
    "\n",
    "\n",
    "@op\n",
    "def get_cf_edited_act(\n",
    "    val: Tensor,\n",
    "    features_to_edit: Tuple[str,...],\n",
    "    base_prompts: Any, # List[Prompt],\n",
    "    cf_prompts: Any, # List[Prompt],\n",
    "    codes: Any, # Dict[Tuple[str,...], Tensor],\n",
    "    method: Literal['mean_ablate_subspace', 'zero_ablate_subspace', 'arithmetic'],\n",
    "    A_ref: Optional[Tensor] = None,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Edit an activation using the given counterfactual prompts in order to infer\n",
    "    the new values for the features being edited.\n",
    "    \n",
    "    - features_to_edit is a tuple representing the features we want to change,\n",
    "    e.g. ('s', 'io_pos',). The new values for these features will be inferred\n",
    "    from the counterfactual prompts. Then, we use the features in the codes to\n",
    "    figure out how to edit the activation.\n",
    "    \"\"\"\n",
    "    code_features = list(codes.keys())\n",
    "    base_feature_idxs = get_prompt_feature_idxs(prompts=base_prompts, \n",
    "                                                features=code_features, )\n",
    "    cf_feature_idxs = get_prompt_feature_idxs(prompts=cf_prompts,\n",
    "                                              features=code_features, )\n",
    "\n",
    "    def turn_tensor_to_tuples(t: Tensor) -> List[Tuple[int,...]]:\n",
    "        return [tuple(x.cpu().tolist()) for x in t]\n",
    "    \n",
    "    base_feature_idxs = {k: turn_tensor_to_tuples(v) for k, v in base_feature_idxs.items()}\n",
    "    cf_feature_idxs = {k: turn_tensor_to_tuples(v) for k, v in cf_feature_idxs.items()}\n",
    "\n",
    "    edited_act = get_edited_act(\n",
    "        val=val,\n",
    "        codes=codes,\n",
    "        feature_idxs_to_delete=base_feature_idxs,\n",
    "        feature_idxs_to_insert=cf_feature_idxs,\n",
    "        A_reference=A_ref,\n",
    "        method=method,\n",
    "    )\n",
    "    return edited_act\n",
    "\n",
    "def get_forced_hook(\n",
    "    prompts: List[Prompt],\n",
    "    node: Node, \n",
    "    A: Tensor,\n",
    ") -> Tuple[str, Callable]:\n",
    "    \"\"\"\n",
    "    Get a hook that forces the activation of the given node to be the given value.\n",
    "    \"\"\"\n",
    "    def hook_fn(activation: Tensor, hook: HookPoint) -> Tensor:\n",
    "        idx = node.idx(prompts=prompts)\n",
    "        activation[idx] = A\n",
    "        return activation\n",
    "    return (node.activation_name, hook_fn)\n",
    "\n",
    "@op\n",
    "def run_activation_patch(\n",
    "    base_prompts: Any, # List[Prompt],\n",
    "    cf_prompts: Any, # List[Prompt],\n",
    "    nodes: List[Node],\n",
    "    activations: List[Tensor],\n",
    "    batch_size: int,\n",
    "    model_id: str = MODEL_ID,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Run a standard activation patch in a batched way\n",
    "    \"\"\"\n",
    "    model = get_model_obj(model_id)\n",
    "    assert all([len(base_prompts) == v.shape[0] for v in activations])\n",
    "    n = len(base_prompts)\n",
    "    n_batches = (n + batch_size - 1) // batch_size\n",
    "    base_logits_list = []\n",
    "    cf_logits_list = []\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        batch_indices = slice(i * batch_size, (i + 1) * batch_size)\n",
    "        prompts_batch = base_prompts[batch_indices]\n",
    "        cf_batch = cf_prompts[batch_indices]\n",
    "        base_dataset = PromptDataset(prompts_batch, model=model)\n",
    "        cf_dataset = PromptDataset(cf_batch, model=model)\n",
    "        hooks = [get_forced_hook(prompts=prompts_batch, node=node, A=act[batch_indices]) for node, act in zip(nodes, activations)]\n",
    "        changed_logits = model.run_with_hooks(base_dataset.tokens, fwd_hooks=hooks)[:, -1, :]\n",
    "        base_answer_logits = changed_logits.gather(dim=-1, index=base_dataset.answer_tokens.cuda())\n",
    "        cf_answer_logits = changed_logits.gather(dim=-1, index=cf_dataset.answer_tokens.cuda())\n",
    "        base_logits_list.append(base_answer_logits)\n",
    "        cf_logits_list.append(cf_answer_logits)\n",
    "    base_logits = torch.cat(base_logits_list, dim=0)\n",
    "    cf_logits = torch.cat(cf_logits_list, dim=0)\n",
    "    return base_logits, cf_logits\n",
    "\n",
    "def get_probability_difference(\n",
    "        logits: Tensor, # of shape (batch, vocab), last token logits\n",
    "        yy_values: List[str],\n",
    "        yy_token_idxs: List[int],\n",
    "        yys: List[str], # of shape (batch, )\n",
    "        ):\n",
    "    \"\"\"\n",
    "    The metric used to discover the circuit in the paper is the *probability*\n",
    "    difference, and we adopt it here as well.\n",
    "    \"\"\"\n",
    "    yy_idx_in_list = yy_values.index(yy)\n",
    "    tokens_gt_yy = Tensor(yy_token_idxs[yy_idx_in_list + 1:]).to(logits.device)\n",
    "    tokens_lte_yy = Tensor(yy_token_idxs[:yy_idx_in_list + 1]).to(logits.device)\n",
    "    logits_gt_yy = logits[:, tokens_gt_yy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training supervised dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editing with the supervised dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editing with the SAEs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
